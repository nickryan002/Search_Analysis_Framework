import csv
from sortedcontainers import SortedDict
import visits_revenue_aggregator
import catalog_match_checker
import shingles_dict_generator
import problematic_query_rollup

# Global Constants for filenames
ENTITY_TABLE_CSV = 'ShingleEntityMatcher/entity_table_new.csv'
MATCHED_TABLE_CSV = 'ShingleEntityMatcher/Output/MatchedTable.csv'
UNMATCHED_TABLE_CSV = 'ShingleEntityMatcher/Output/UnmatchedTable.csv'
LULU_TERMS_CSV = 'ClientData/lululemon search terms - may-aug.csv'
LULU_TERMS_AGGREGATED_CSV = 'ShingleEntityMatcher/lulu_terms_Aggregated.csv'
SYNONYMS_TXT = 'ShingleEntityMatcher/lulu_solr_synonyms.txt'
SYNONYM_MATCHES_CSV = 'ShingleEntityMatcher/SynonymExpansions.csv'
PROBLEMATIC_SEARCHES_CSV = 'ShingleEntityMatcher/Output/potentially_problematic_searches.csv'
ROLLED_UP_PROBLEMATIC_SEARCHES_CSV = 'ShingleEntityMatcher/Output/rolled_up_searches.csv'

# Global SortedDict to store shingles with their corresponding details
shingles_dict = SortedDict()

def write_dict_to_file(dictionary: SortedDict, file_name: str) -> None:
    """
    Write the sorted dictionary to a text file.

    Args:
        dictionary (SortedDict): The dictionary to write.
        file_name (str): The name of the file to write to.
    """
    with open(file_name, 'w') as file:
        for key, value in dictionary.items():
            file.write(f'{key}: {value}\n')

def write_to_matched_unmatched_csvs(search_query: str, shingles: list, visits: str, revenue: str) -> None:
    """
    Write matched and unmatched shingles to their respective CSV files.

    Args:
        search_query (str): The search query.
        shingles (list): List of shingles to process.
        visits (str): Number of visits for the query.
        revenue (str): Revenue generated by the query.
    """
    with open(MATCHED_TABLE_CSV, mode='a', newline='', encoding='utf-8') as matched_file, \
         open(UNMATCHED_TABLE_CSV, mode='a', newline='', encoding='utf-8') as unmatched_file:
        
        matched_writer = csv.writer(matched_file)
        unmatched_writer = csv.writer(unmatched_file)
        
        for shingle in shingles:
            lower_shingle = shingle.lower()
            if lower_shingle in shingles_dict:
                write_matched_shingles(matched_writer, shingle, search_query, visits, revenue)
            else:
                unmatched_writer.writerow([shingle, search_query, visits, revenue])

def write_matched_shingles(writer: csv.writer, shingle: str, search_query: str, visits: str, revenue: str) -> None:
    """
    Write matched shingles to the matched CSV file.

    Args:
        writer (csv.writer): CSV writer object.
        shingle (str): Shingle to write.
        search_query (str): Search query associated with the shingle.
        visits (str): Number of visits for the query.
        revenue (str): Revenue generated by the query.
    """
    lower_shingle = shingle.lower()
    matched_entities = {}
    partial_matches = []

    for entry in shingles_dict[lower_shingle]:
        entity = entry[0]
        entity_type = entry[2]
        matched_entities.setdefault(entity_type, []).append(entity)
        if entry[1] == "partial":
            partial_matches.append(entity)

    entity_overlap = len({item for sublist in matched_entities.values() for item in sublist}) > 1
    entity_type_overlap = len(matched_entities) > 1

    entity_types_str = '|'.join(matched_entities.keys())
    partial_matches_str = '|'.join(partial_matches) if partial_matches else shingle

    writer.writerow([
        shingle, partial_matches_str, "partial" if partial_matches else "full", 
        entity_types_str, search_query, visits, revenue,
        "Y" if entity_overlap or entity_type_overlap else "N",
        len({item for sublist in matched_entities.values() for item in sublist}),
        len(matched_entities)
    ])

def initialize_csvs() -> None:
    """
    Initialize the MatchedTable, UnmatchedTable, and ProblematicSearches CSVs with headers.
    """
    with open(MATCHED_TABLE_CSV, mode='w', newline='', encoding='utf-8') as matched_file, \
         open(UNMATCHED_TABLE_CSV, mode='w', newline='', encoding='utf-8') as unmatched_file, \
         open(PROBLEMATIC_SEARCHES_CSV, mode='w', newline='', encoding='utf-8') as problematic_file:
        
        matched_writer = csv.writer(matched_file)
        unmatched_writer = csv.writer(unmatched_file)
        problematic_writer = csv.writer(problematic_file)
        
        matched_writer.writerow([
            "Matched Shingle", "Entities", "Shingle Type", "Entity Type", "Search Query", 
            "Visits", "Revenue", "Overlap", "Entity Overlaps", "Entity Type Overlaps"
        ])
        unmatched_writer.writerow(["Unmatched Shingle", "Search Query", "Visits", "Revenue"])
        problematic_writer.writerow([
            "Problematic Search Query", "Legitimate", "Catalog Field", 
            "Normalization Filters", "Visits", "Revenue"
        ])

def extract_dict_info(tokens: list, info_type: str) -> str:
    """
    Extract information (entity type or filter) from the shingles dictionary based on tokens.

    Args:
        tokens (list): List of tokens to extract information from.
        info_type (str): Type of information to extract ("entity_type" or "filter").

    Returns:
        str: A string representation of the extracted information.
    """
    info_to_tokens = {}

    for token in tokens:
        if token in shingles_dict:
            lists = shingles_dict[token]
            for sublist in lists:
                key_index = 2 if info_type == "entity_type" else -1
                if sublist[key_index]:
                    info_to_tokens.setdefault(sublist[key_index], set()).add(token)

    # Create the final string with unique tokens in parentheses
    final_result = "/".join([f"{key}({ '_'.join(sorted(info_to_tokens[key])) })" for key in info_to_tokens])

    return final_result


def process_search_queries() -> None:
    """
    Process search queries from the aggregated terms CSV and populate matched/unmatched tables.
    """
    # Open the aggregated search terms CSV for reading and the problematic searches CSV for appending
    with open(LULU_TERMS_AGGREGATED_CSV, mode='r', newline='', encoding='utf-8') as search_queries_file, \
         open(PROBLEMATIC_SEARCHES_CSV, mode='a', newline='', encoding='utf-8') as problematic_file:
        
        # Create a CSV reader to process the input search queries
        reader = csv.DictReader(search_queries_file)

        # Check for and remove any Byte Order Mark (BOM) from the first field name, if present
        if reader.fieldnames and reader.fieldnames[0].startswith('\ufeff'):
            reader.fieldnames[0] = reader.fieldnames[0].replace('\ufeff', '')

        # Initialize the CSV writer for the problematic searches file
        problematic_writer = csv.writer(problematic_file)
        print("Processing search queries...")

        # Iterate over each row in the aggregated search terms CSV
        for row in reader:
            search_phrase = row['Search Query']  # Extract the search query phrase
            visits = row['Visits']              # Extract the number of visits
            revenue = row['Revenue']            # Extract the associated revenue

            # Generate shingles (substrings) from the search phrase
            shingles = shingles_dict_generator.generate_shingles(search_phrase)
            
            # Write the search phrase and related info to matched/unmatched CSVs
            write_to_matched_unmatched_csvs(search_phrase, shingles, visits, revenue)

            # Tokenize the search phrase and filter tokens present in the shingles dictionary
            tokens = search_phrase.split()
            tokens_in_dict = [token for token in tokens if token.lower() in shingles_dict]

            # Extract entity types associated with the tokens
            entity_types = extract_dict_info(tokens, "entity_type")

            # Check if the search phrase has more than one token and more than one entity type
            if len(tokens) > 1 and len(entity_types.split("/")) > 1 and tokens == tokens_in_dict:        
                
                # If unnormalized values in the row do not match the catalog, mark as problematic
                if not catalog_match_checker.check_unnormalized_values_in_row(tokens_in_dict, shingles_dict):
                    problematic_query_row = [search_phrase, "N", entity_types, "", visits, revenue]

                    # If normalized values match, update the legitimacy and filter info accordingly
                    if catalog_match_checker.check_normalized_values_in_row(tokens_in_dict, shingles_dict):
                        problematic_query_row = [
                            search_phrase, "Y", entity_types, 
                            extract_dict_info(tokens_in_dict, "filter"), visits, revenue
                        ]
                    else:
                        problematic_query_row = [
                            search_phrase, "N", entity_types, 
                            extract_dict_info(tokens_in_dict, "filter"), visits, revenue
                        ]

                    # Write the problematic search query to the problematic searches CSV
                    problematic_writer.writerow(problematic_query_row)
                    print(f"Processed search query: {search_phrase}")

        print("Finished processing all search queries.")

def main() -> None:
    """
    Main function to execute the pipeline for processing search queries and writing results.
    """
    shingles_dict_generator.read_csv_and_populate_shingles_dict(ENTITY_TABLE_CSV, shingles_dict)
    write_dict_to_file(shingles_dict, 'ShingleEntityMatcher/dictionary.txt')
    #visits_revenue_aggregator.normalize_and_aggregate(LULU_TERMS_CSV, LULU_TERMS_AGGREGATED_CSV)
    initialize_csvs()
    process_search_queries()
    problematic_query_rollup.rollup_queries(PROBLEMATIC_SEARCHES_CSV, ROLLED_UP_PROBLEMATIC_SEARCHES_CSV)

if __name__ == "__main__":
    main()
    